# 技术咨询：ReAct + Critical-step DPO 训练方案收敛性评估

> **咨询目的**: 请同学评估该训练方案是否可行，是否存在收敛性问题
> **创建时间**: 2026-01-28
> **期望反馈**: 方案可行性、潜在问题、改进建议

---

## 1. 问题抽象

### 1.1 任务描述（通用化）

我们正在构建一个 **多步检索决策 Agent**，其核心能力是：

> 给定一个用户问题，Agent 需要**动态决定**使用哪些检索工具、以什么顺序执行、何时停止检索，最终基于检索结果生成答案。

这是一个典型的 **Tool-use Agent** 问题，检索工具集合是固定的，但调用顺序和次数需要根据中间结果动态决定。

### 1.2 核心挑战

| 挑战 | 描述 |
|------|------|
| **动态决策** | 每一步的最优 action 依赖于前序 observations |
| **停止判断** | 需要学会判断"信息何时充分" |
| **策略组合** | 复杂问题可能需要多个工具协作 |

---

## 2. 可用工具集（5 种检索策略）

| 工具 ID | 名称 | 功能 | 适用场景 |
|---------|------|------|----------|
| `T1` | `vector_single_hop` | 单跳向量检索 | 简单事实查询 |
| `T2` | `graph_augmented` | 图结构增强检索 | 实体关系查询 |
| `T3` | `temporal_multi_hop` | 时序多跳检索 | 事件演变追踪 |
| `T4` | `session_aware` | 会话上下文检索 | 多轮对话、指代消解 |
| `T5` | `stop` | 停止检索 | 信息充分时终止 |

**约束**:
- 每轮只能选择 1 个工具
- 最大检索轮次: 5
- 选择 `stop` 后进入答案生成阶段

---

## 3. 训练方案：ReAct + Critical-step DPO

### 3.1 ReAct 框架

采用 ReAct (Reasoning + Acting) 模式，Agent 输出格式为：

```
[Thought] 当前状态分析和推理
[Action] T1 | T2 | T3 | T4 | T5
[Observation] (环境返回的检索结果)
... (循环直到 Action = T5)
[Answer] 最终答案
```

### 3.2 两阶段训练

```
┌─────────────────────────────────────────────────────────────┐
│  Stage 1: SFT (Supervised Fine-Tuning)                      │
│                                                             │
│  目标: 学习 ReAct 格式，建立基础 tool-use 能力              │
│  数据: ~300 条完整 trajectory                               │
│  格式: (Question, Full_Trajectory) pairs                    │
│                                                             │
│  示例:                                                      │
│  Q: "A和B的关系如何演变？"                                  │
│  Trajectory:                                                │
│    [Thought] 关系+演变，需要图检索和时序检索                │
│    [Action] T2                                              │
│    [Obs] 找到 A-B 是朋友关系                                │
│    [Thought] 有关系类型，缺演变信息                         │
│    [Action] T3                                              │
│    [Obs] 找到关系在时间线上的变化                           │
│    [Thought] 信息充分                                       │
│    [Action] T5                                              │
│    [Answer] A和B从初识发展为朋友...                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  Stage 2: Critical-step DPO                                 │
│                                                             │
│  目标: 优化关键决策点的 action 选择                         │
│  数据: ~500 条 (prompt, chosen, rejected) pairs             │
│                                                             │
│  关键决策点 (Critical Steps):                               │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ D1: 首次工具选择                                    │   │
│  │     给定问题，选择第一个检索工具                    │   │
│  │                                                     │   │
│  │ D2: 继续/停止判断                                   │   │
│  │     给定当前检索结果，决定继续检索还是停止          │   │
│  │                                                     │   │
│  │ D3: 工具切换                                        │   │
│  │     当前工具效果不佳时，选择替代工具                │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  DPO 数据格式:                                              │
│  {                                                          │
│    "prompt": "[Question] + [History] + [Current Obs]",      │
│    "chosen": "正确的 [Thought] + [Action]",                 │
│    "rejected": "错误的 [Thought] + [Action]"                │
│  }                                                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 3.3 DPO 数据示例

#### D1: 首次工具选择

```json
{
  "prompt": "[Question] A和B的关系是什么？\n\n请选择第一个检索工具:",

  "chosen": "[Thought] 这是关系查询问题，需要图结构来找实体间连接\n[Action] T2",

  "rejected": "[Thought] 直接搜索试试\n[Action] T1"
}
```

#### D2: 继续/停止判断

```json
{
  "prompt": "[Question] A和B的关系如何演变？\n[History] T2 → 找到朋友关系\n[Observation] 有关系类型，无时间演变信息\n\n判断是否继续:",

  "chosen": "[Thought] 问题问的是'如何演变'，当前只有静态关系，缺少时序信息\n[Action] T3",

  "rejected": "[Thought] 已经找到关系了，可以回答\n[Action] T5"
}
```

```json
{
  "prompt": "[Question] A和B的关系如何演变？\n[History] T2 → 朋友关系, T3 → 时序演变\n[Observation] 已有关系类型和演变过程\n\n判断是否继续:",

  "chosen": "[Thought] 问题的两个方面都已覆盖，信息充分\n[Action] T5",

  "rejected": "[Thought] 再多搜一些细节\n[Action] T1"
}
```

#### D3: 工具切换

```json
{
  "prompt": "[Question] 事件X的发展过程？\n[History] T1 → 只找到零散片段\n[Observation] 单跳检索未能找到完整信息\n\n选择下一步:",

  "chosen": "[Thought] 单跳效果不好，事件发展需要时序多跳\n[Action] T3",

  "rejected": "[Thought] 再用同样方法试一次\n[Action] T1"
}
```

---

## 4. 咨询问题

### 4.1 核心问题

**Q1: 该训练方案是否能收敛？**

具体关注点：
- SFT 阶段学习 ReAct 格式是否可行？
- Critical-step DPO 是否能有效优化决策点？
- 两阶段训练是否会产生分布偏移问题？

### 4.2 关于 DPO 的疑问

**Q2: DPO 应用于 ReAct 的合理性**

- DPO 原设计用于 sequence-level 偏好，我们将其应用于 step-level 决策是否合理？
- 每个决策点的 (chosen, rejected) 是否足够区分？
- 是否需要考虑 trajectory 级别的 DPO 而非 step 级别？

### 4.3 关于数据的疑问

**Q3: 训练数据量是否足够？**

| 阶段 | 数据量 | 是否充足？ |
|------|--------|------------|
| SFT | ~300 trajectories | ? |
| DPO | ~500 decision pairs | ? |

- 对于 5 个工具的选择空间，这个数据量是否足够覆盖？
- 是否需要数据增强？

### 4.4 关于替代方案

**Q4: 是否有更好的训练方法？**

我们考虑过的替代方案：

| 方案 | 描述 | 顾虑 |
|------|------|------|
| **纯 SFT** | 只做 SFT，不做 DPO | 可能无法学好停止判断 |
| **Trajectory DPO** | 比较完整轨迹而非单步 | 信用分配困难 |
| **GRPO** | 在线 RL，用检索成功率做奖励 | 计算成本高 |
| **分层架构** | 拆分为 Planner + Evaluator | 失去动态调整能力 |

是否有其他推荐的训练方法？

---

## 5. 补充信息

### 5.1 基座模型

- 计划使用: Qwen2.5-7B 或类似规模的中文模型
- 硬件: Apple M4 Pro, 24GB 统一内存

### 5.2 评估指标

| 指标 | 定义 |
|------|------|
| **工具选择准确率** | 首次工具选择是否正确 |
| **停止判断准确率** | 是否在正确时机停止 |
| **轨迹效率** | 平均检索轮次 (越少越好) |
| **端到端准确率** | 最终答案是否正确 |

### 5.3 已有参考

- ReAct 原论文: Yao et al., 2022
- DPO 原论文: Rafailov et al., 2023
- Toolformer: Schick et al., 2023

---

## 6. 期望反馈

1. **可行性判断**: 该方案是否合理？是否存在明显问题？
2. **收敛性风险**: 是否有可能不收敛？风险因素是什么？
3. **改进建议**: 如何调整可以提高成功率？
4. **替代方案**: 是否有更成熟的 Tool-use Agent 训练范式？

---

感谢审阅！期待您的反馈。
